{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f3b0be66",
   "metadata": {},
   "source": [
    "# `Embeddings`\n",
    "\n",
    "<br>\n",
    "\n",
    "\n",
    "Embeddings são representações vetoriais numéricas densas de objetos e relacionamentos do mundo real. Existem várias formas de representar vetorialmente um objeto ou uma categoria. A mais utilizada delas é o One Hot Encoding, que representa categorias em vetores binários.\n",
    "\n",
    "Entretanto, embeddings são mais interessantes, pois são capazes de trazer a nuance de similaridade entre objetos semelhantes: a representação vetorial de objetos parecidos será muito próxima no espaço vetorial.\n",
    "\n",
    "Vamos entender isso passo a passo!\n",
    "\n",
    "\n",
    "![alt_text](https://www.pinecone.io/images/vector_embeddings.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "752d72c8",
   "metadata": {},
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4122c193",
   "metadata": {},
   "source": [
    "### `One-Hot Encoding`\n",
    "\n",
    "\n",
    "\n",
    "O one-hot encoding é um método simples de representar variáveis categóricas. É um algoritmo não supervisionado que faz um de-para de uma categoria para um vetor, onde os valores possíveis são 0 ou 1. \n",
    "\n",
    "Assim, é criado um vetor para cada categoria, do tamanho do número de categorias, onde as posições são iguais a 0 se a observação não possui aquela categoria, e 1 se possui.\n",
    "\n",
    "\n",
    "A grande desvantagem desse método é que categorias que possuem similaridade na vida real, não terão nenhum tipo de similaridade no vetor de espaços, pois todas as categorias estão equidistantes. \n",
    "\n",
    "\n",
    "\n",
    "![alt_text](https://miro.medium.com/max/1400/1*ggtP4a5YaRx6l09KQaYOnw.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "065c6568",
   "metadata": {},
   "source": [
    "<br><br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18c9a300",
   "metadata": {},
   "source": [
    "## `Vantagens dos Embedings`\n",
    "\n",
    "<br>\n",
    "\n",
    "Por outro lado, um embedding captura parte dos objetos, como textos, colocando textos semelhantes próximos uns dos outros no espaço embedding. Um embedding pode ser aprendido e reutilizada entre os modelos.\n",
    "\n",
    "\n",
    "\n",
    "![alt_text](https://cdn.openai.com/embeddings/draft-20220124e/vectors-mobile-1.svg)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2592aa24",
   "metadata": {},
   "source": [
    "<br><br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd0a4537",
   "metadata": {},
   "source": [
    "## `Word2Vec`\n",
    "\n",
    "\n",
    "\n",
    "Word2vec gera embeddings de palavras. As palavras são codificadas em vetores one-hot e alimentadas em uma camada oculta de uma rede neural, que gera pesos ocultos. Esses pesos ocultos são usados para prever outras palavras próximas. \n",
    "\n",
    "Embora esses pesos ocultos sejam usados para treinamento, o word2vec não os usará para a tarefa em que foi treinado. Em vez disso, os pesos ocultos são retornados como os próprios embeddings.\n",
    "\n",
    "<br>\n",
    "\n",
    "![alt_text](https://opendatascience.com/wp-content/uploads/2018/11/Screen-Shot-2018-11-14-at-1.42.58-AM-768x461.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea65ceba",
   "metadata": {},
   "source": [
    "<br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "206985a9",
   "metadata": {},
   "source": [
    "#### `Semântica`\n",
    "\n",
    "\n",
    "As palavras encontradas em contextos semelhantes terão embeddings parecidos. Além disso, os embeddings podem ser usados para formar analogias. Por exemplo, o vetor do rei ao homem é muito semelhante ao da rainha à mulher.\n",
    "\n",
    "![alt_text](https://miro.medium.com/max/678/1*5F4TXdFYwqi-BWTToQPIfg.jpeg)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40a04a14",
   "metadata": {},
   "source": [
    "<br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec48788e",
   "metadata": {},
   "source": [
    "#### `Desvantagem`\n",
    "\n",
    "\n",
    "\n",
    "Um problema com Word2Vec é que palavras únicas têm um mapeamento de vetor. Isso significa que todos os usos semânticos de uma palavra são combinados em uma representação. \n",
    "\n",
    "Por exemplo, a palavra “jogar” em “vou me jogar nesse curso” e “eu vou jogar bola” terá o mesmo embedding, sem a capacidade de distinguir o contexto.\n",
    "\n",
    "\n",
    "Além disso, é um algoritmo custoso, que precisa de bastante recurso para ser treinado para grandes conjuntos de dados."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
